{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6573,"status":"ok","timestamp":1679147656532,"user":{"displayName":"汤济玮","userId":"17722932606773371846"},"user_tz":-480},"id":"YSpVZs053W_7","outputId":"cf22d998-b5ee-4374-c60c-4d6972740d56"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAPYmRCi3edf"},"outputs":[],"source":["import os\n","path = '/content/drive/MyDrive/'\n","os.chdir(path)"]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","df = pd.read_csv(\"./Processed_Daytime/merge_train_test.csv\")\n","correct_names = []\n","for image_name in os.listdir(\"./Processed_Daytime/images_rotate\"):\n","  correct_names.append(image_name)\n","# print(correct_names)\n","# for line in df[['file_Id', \"PM2.5\"]]:\n","# print(df[['file_Id', \"PM2.5\"]])\n","with open(\"./Processed_Daytime/merge_train_test_final.csv\", mode=\"w\") as file:\n","  # 写入表头\n","  file.write('file_Id,PM2.5\\n')\n","\n","  for index in range(len(df)):\n","    temp_str = str(df.loc[index, ['file_Id']]).split(\" \")[4]\n","    file_Id = temp_str.split('\\n')[0]\n","    temp_PM = str(df.loc[index, ['PM2.5']]).split(\" \")[4]\n","    PM = temp_PM.split('\\n')[0]\n","    file.write(f'{file_Id},{PM}\\n')\n","  # print(PM)\n","  # if file_Id in correct_names:\n","  #   df.loc[index].to_csv(\"./Processed_Daytime/merge_train_test_final.csv\", mode='a+')\n","print(\"Finished\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0F8qLaeoI1IS","executionInfo":{"status":"ok","timestamp":1679151883407,"user_tz":-480,"elapsed":10073,"user":{"displayName":"汤济玮","userId":"17722932606773371846"}},"outputId":"b649d74c-c77d-4f3b-92cc-c30ccd5a0a68"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished\n"]}]},{"cell_type":"code","execution_count":86,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1679151885795,"user":{"displayName":"汤济玮","userId":"17722932606773371846"},"user_tz":-480},"id":"s-kDY7G1Sq5o","outputId":"d83e3ab9-8669-4717-9cdd-c4457f8c2b26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Finished!\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv(\"./Processed_Daytime/merge_train_test_final.csv\")\n","X_train, X_test, y_train, y_test = train_test_split(df['file_Id'], df['PM2.5'], test_size=0.2)\n","df_test = pd.DataFrame(columns=['file_Id', 'PM2.5'])\n","df_test['file_Id'] = X_test\n","df_test['PM2.5'] = y_test\n","# print(df_test)\n","df_train = pd.DataFrame(columns=['file_Id', 'PM2.5'])\n","df_train['file_Id'] = X_train\n","df_train['PM2.5'] = y_train\n","\n","train_csv_dir = r\"/content/drive/MyDrive/Processed_Daytime/anno/processed_train.csv\"\n","test_csv_dir = r\"/content/drive/MyDrive/Processed_Daytime/anno/processed_test.csv\"\n","\n","df_test.to_csv(test_csv_dir)\n","df_train.to_csv(train_csv_dir)\n","print(\"Finished!\")"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"J98ljOiyB7It","executionInfo":{"status":"error","timestamp":1679148980778,"user_tz":-480,"elapsed":305195,"user":{"displayName":"汤济玮","userId":"17722932606773371846"}},"outputId":"05496ff0-93a4-4576-8187-9712d20f6aec"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-822a57d091ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-64-822a57d091ce>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m       \u001b[0mresnet_101\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-64-822a57d091ce>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# print(img_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mimg_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDarkChannel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0msaturation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaturation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mimg_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-64-822a57d091ce>\u001b[0m in \u001b[0;36mDarkChannel\u001b[0;34m(im, win)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2BGR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mkernal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetStructuringElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMORPH_RECT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 0)"]}],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import math\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import time\n","import os\n","from torchvision import models\n","import cv2\n","import PIL\n","from PIL import Image\n","import cv2\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","\n","def DarkChannel(im, win=15):\n","  \"\"\"求暗通道图\n","  input:three channels rgb, window size of fusing image\n","  output:dark channel image\n","  \"\"\"\n","  if isinstance(im, PIL.Image.Image):\n","    im = cv2.cvtColor(np.asarray(im), cv2.COLOR_RGB2BGR)\n","    \n","  b, g, r = cv2.split(im)\n","  dc = cv2.min(cv2.min(r, g), b)\n","  kernal = cv2.getStructuringElement(cv2.MORPH_RECT, (win, win))\n","  dark = cv2.erode(dc, kernal)\n","  return Image.fromarray(dark, mode='L')\n","\n","def Saturation(im):\n","  im = Image.fromarray(im)\n","  img_hsv = im.convert('HSV')\n","  h, s, v = img_hsv.split()\n","  return s\n","\n","class ImgDataset(Dataset):\n","  def calc_image_entropy(self, img):\n","    entropys = []\n","\n","    hist = cv2.calcHist([img], [0], None, [256], [0, 255])\n","    total_pixels = img.shape[0] * img.shape[1]\n","\n","    for frequency in hist:\n","      p = frequency / total_pixels\n","      if p == 0:\n","        pixel_entropy = 0\n","      else:\n","        pixel_entropy = -1 * p * (np.log(p) / np.log(2))\n","      entropys.append(pixel_entropy)\n","    \n","    return np.sum(entropys)\n","\n","  # 调整图像的亮度，以区分效果好的图和不好的图\n","  def tunning_img_light(self, img, w):\n","    img_ = img.copy().astype(np.int16)\n","\n","    bias = np.ones_like(img_)\n","    bias *= 255\n","    \n","    temp = np.add(w * np.subtract(img_, bias), bias)\n","    img_ = np.minimum(temp, bias)\n","    \n","    img_ = np.clip(img_, 0, 255)\n","    return img_.astype(np.uint8)\n","\n","  def get_info(self, data_dir):\n","    data_info = list()\n","    judge = data_dir.split(\"/\")[-1]\n","    if judge == \"train\":\n","      path = r\"/content/drive/MyDrive/Processed_Daytime/anno/processed_train.csv\"\n","    else:\n","      path = r\"/content/drive/MyDrive/Processed_Daytime/anno/processed_test.csv\"\n","    df = pd.read_csv(path)\n","    df_xy = df[[\"file_Id\", \"PM2.5\"]]\n","    for index in range(len(df_xy)):\n","      temp_str = str(df_xy.loc[index, [\"file_Id\"]]).split(\" \")[4]\n","      final_path = temp_str.split('\\n')[0]\n","      img_path = os.path.join(data_dir, final_path)\n","      label = df_xy.loc[index, [\"PM2.5\"]]\n","      # normalization\n","      label = (label - 1) / (262 - 1)      \n","      data_info.append((img_path, label))\n","    return data_info\n","\n","  def __init__(self, data_dir, transforms=None):\n","    self.data_info = self.get_info(data_dir)\n","    self.transforms = transforms\n","    \n","  def __getitem__(self, index):\n","    features = list()\n","    labels = list()\n","    w = list(range(1, 21))\n","    img_path, label = self.data_info[index]\n","    labels.append(float(label))\n","    # print(img_path)\n","    img_ = cv2.imread(img_path)\n","    dc = DarkChannel(img_)\n","    saturation = Saturation(img_)\n","    img_ = np.stack([dc, saturation])\n","    for element in w:\n","      dc_entropy = self.calc_image_entropy(self.tunning_img_light(img_[0], element))[0]\n","      saturation_entropy = self.calc_image_entropy(self.tunning_img_light(img_[0], element))[0]\n","      features.append(dc_entropy)\n","      features.append(saturation_entropy)\n","    # 转换数据类型\n","    # features = torch.from_numpy(np.array(features))\n","    features = Image.fromarray(np.array(features).astype(np.uint8))\n","    features = features.convert('RGB')\n","    w, h = features.size\n","    # print(\"图像宽度：\", w)\n","    # print(\"图像高度：\", h)\n","    features = self.transforms(features)\n","    w_, h_ = features.shape[-2:]\n","    # print(\"转换后图像宽度：\", w_)\n","    # print(\"转换后图像高度：\", h_)\n","    labels = torch.from_numpy(np.array(labels))\n","    # print(labels.shape)\n","    return features, labels    \n","  \n","  def __len__(self):\n","    return len(self.data_info)\n","\n","# path = r\"/content/drive/MyDrive/Daytime/merge_train_test.csv\"\n","# df = pd.read_csv(path)\n","# X_train, X_test, y_train, y_test = train_test_split(df['file_Id'], df['PM2.5'], test_size=0.2)\n","# print(X_train)\n","# print(\"------------------\")\n","# print(y_train)\n","\n","transforms = transforms.Compose(\n","    [\n","      transforms.Resize((224, 224)),\n","      transforms.ToTensor()\n","    ]\n",")\n","\n","train_dir = r\"/content/drive/MyDrive/Processed_Daytime/train\"\n","test_dir = r\"/content/drive/MyDrive/Processed_Daytime/test\"\n","\n","train_data = ImgDataset(data_dir=train_dir,transforms=transforms)\n","test_data = ImgDataset(data_dir=test_dir,transforms=transforms)\n","\n","# 做32次___getitem__()\n","train_data_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n","test_data_loader = DataLoader(dataset=test_data, batch_size=32)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","resnet_101 = models.resnet101(pretrained=True)\n","resnet_101.fc = nn.Linear(2048, 1)\n","resnet_101 = resnet_101.to(device)\n","\n","EPOCHS = 100\n","LR = 1e-4\n","\n","loss_func = nn.MSELoss()\n","loss_func = loss_func.to(device)\n","optimizer = optim.Adam(resnet_101.parameters(), lr=LR)\n","\n","# --------------record the loss and accuracy----------\n","total_train_loss = []\n","\n","def train():\n","  for epoch in range(EPOCHS):\n","      train_loss = 0.0\n","      resnet_101.train()\n","      for images, labels in train_data_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        # print(labels)\n","        if hasattr(torch.cuda, 'empty_cache'):\n","\t        torch.cuda.empty_cache()\n","        \n","        outputs = resnet_101(images)\n","        # print(outputs.shape)\n","        outputs = outputs.type(torch.float64)\n","        # print(outputs)\n","        # print(\"-------\")\n","        # print(labels)\n","        loss = loss_func(outputs, labels)\n","        \n","        # 反向传播前要把上一次的梯度清0\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","      print(\"Epoch {}, Training loss: {:.6f}\".format(epoch, train_loss))\n","      total_train_loss.append(train_loss)\n","\n","def test():\n","  with torch.no_grad():\n","    test_loss = 0.0\n","    resnet_101.eval()\n","    for images, labels in test_data_loader:\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      \n","      outputs = resnet_101(images)\n","      loss = loss_func(outputs, images)\n","      test_loss += loss\n","    print(\"The test loss: {:.6f}\".format(test_loss))  \n","\n","if __name__ == \"__main__\":\n","  train()\n","  test()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyMUp6kJVLS6ry3YgXT/QtPL"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}